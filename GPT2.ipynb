{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavkarnani/StoryGenerator/blob/Vanilla-GPT-2/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5KREO24oOZFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "id": "otcuzrGa_DPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import csv"
      ],
      "metadata": {
        "id": "fX3OzD4x5aWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler"
      ],
      "metadata": {
        "id": "la1QjLYotub0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "-5MmNRCIDmCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "R35k7TJzAjkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 73\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "EPOCHS = 4\n",
        "SAMPLE_EVERY = 10000\n",
        "\n",
        "MAX_INPUT_SEQUENCE_LENGTH = 600"
      ],
      "metadata": {
        "id": "dxR6Jl7SMDjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>', 'sep_token': '<SEP>'}\n",
        "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)"
      ],
      "metadata": {
        "id": "Z9yFEujlNTJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/refined.csv\")"
      ],
      "metadata": {
        "id": "oWoEygXaoaRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer)"
      ],
      "metadata": {
        "id": "BeycgWHn78YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[0, 'storyline']"
      ],
      "metadata": {
        "id": "muxDXs-MtesT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StoryOutlineDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, tokenizer, max_input_length):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        self.data = data\n",
        "        self.labels_attn = []\n",
        "\n",
        "        for i in tqdm(range(len(self.data))):\n",
        "            text = self.data.loc[i, 'text']\n",
        "            outline = self.data.loc[i, 'storyline'].split(' ')\n",
        "            outline = \" \".join(outline[:100]).replace(\"<SEP>\", \"\")\n",
        "\n",
        "            input = outline + \"<SEP>\" + text\n",
        "\n",
        "            encodings_dict_story = tokenizer('<BOS> ' + input + ' <EOS>',\n",
        "                                     truncation=True,\n",
        "                                     max_length=max_input_length,\n",
        "                                     padding=True\n",
        "                                    )\n",
        "            \n",
        "            self.input_ids.append(torch.tensor(encodings_dict_story['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict_story['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        return self.input_ids[ind], self.attn_masks[ind]"
      ],
      "metadata": {
        "id": "8h7-0Lz2OVhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story_dataset = StoryOutlineDataset(data.loc[0:50000], tokenizer, MAX_INPUT_SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "Lea6BixqOcuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "v14hnJgQSGpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split(split, dataset):\n",
        "    train_size = int(split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    return train_size, val_size"
      ],
      "metadata": {
        "id": "zU3HmHa6R7q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size, val_size = train_val_split(0.8, story_dataset)\n",
        "train_dataset, val_dataset = random_split(story_dataset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "FQeIzcVwQPqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "W1RRLycNSwWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = True)"
      ],
      "metadata": {
        "id": "FcM1oCXwS0KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "eps = 1e-8\n",
        "warmup_steps = 100"
      ],
      "metadata": {
        "id": "UjkTwpc4TNZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"<SEP>\")"
      ],
      "metadata": {
        "id": "CHzKDRDfbOd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configuration = GPT2Config(vocab_size=len(tokenizer), n_positions = MAX_INPUT_SEQUENCE_LENGTH, \n",
        "                           activation_function = \"gelu_new\", resid_pdrop = 0.1, embd_pdrop = 0.2,\n",
        "                           attn_pdrop = 0.2, output_attentions = True, output_hidden_states = True)\n",
        "\n",
        "model_config = configuration.from_pretrained('gpt2', output_hidden_states=True)"
      ],
      "metadata": {
        "id": "JcGYg_pnTgYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.config = model_config\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "w-0Ex9oQU6Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "i5pFUrjmVhMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "metadata": {
        "id": "KR4RqJTTVbEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/model.pth'))"
      ],
      "metadata": {
        "id": "s37KNumxtnzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model.cuda()\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=eps)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 100,\n",
        "                                                                 T_mult = 3,\n",
        "                                                                 eta_min = 1e-7)"
      ],
      "metadata": {
        "id": "SpVtl6i_syjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_loss = nn.MSELoss()"
      ],
      "metadata": {
        "id": "itbRYkZ4IJaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_out_texts(text):\n",
        "    t_map = tokenizer.special_tokens_map\n",
        "    for key in t_map:\n",
        "        text = text.replace(t_map[key], '')\n",
        "    return text\n",
        "\n",
        "def inference(val_loader):\n",
        "    model.eval()\n",
        "\n",
        "    for i, batch in enumerate(val_loader):\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            lens = np.array([])\n",
        "            input_ids = batch[0].numpy()\n",
        "            attn_masks = batch[1].numpy()\n",
        "\n",
        "            truncated_input = []\n",
        "            truncated_attention_mask = []\n",
        "            for i, input_id in enumerate(input_ids):\n",
        "                context_index = np.where(input_id == 50260)[0][0]\n",
        "                truncated_input.append(input_id[:context_index+1])\n",
        "                truncated_attention_mask.append(attn_masks[i][:context_index+1])\n",
        "                lens = np.append(lens, context_index+1)\n",
        "    \n",
        "            max_len = int(np.amax(lens))\n",
        "\n",
        "            padded_tokens = []\n",
        "            for tok_ids in truncated_input:\n",
        "                \n",
        "                padded_tokens.append(list(tok_ids) + [0] * (max_len - len(tok_ids)))\n",
        "                \n",
        "            padded_tokens = torch.LongTensor(padded_tokens).to(device)\n",
        "            attn_mask = np.zeros(padded_tokens.shape)\n",
        "            \n",
        "            for ix, lengths in enumerate(lens):\n",
        "                print(ix)\n",
        "                print(lengths)\n",
        "                attn_mask[ix][:int(lengths)] = 1\n",
        "\n",
        "            attn_mask = torch.tensor(attn_mask).long().to(device)\n",
        "\n",
        "    story_ids = model.generate(padded_tokens, attention_mask=attn_mask,\n",
        "                            num_beams=5,\n",
        "                            max_length=800,\n",
        "                            temperature=0.9,\n",
        "                            remove_invalid_values = True,\n",
        "                            top_k=50,\n",
        "                            do_sample=True)\n",
        "\n",
        "    raw_stories = [tokenizer.decode(story) for story in story_ids]\n",
        "    output_texts = list(map(format_out_texts, raw_stories))\n",
        "    print(output_texts)\n",
        "    return output_texts"
      ],
      "metadata": {
        "id": "6f_luGaPtXva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(ep, train_loader):\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_loader)):\n",
        "\n",
        "        model.train() \n",
        "\n",
        "        b_input_ids = batch[0]\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        labels = b_input_ids.clone().numpy()\n",
        "\n",
        "        for i, text in enumerate(b_input_ids.numpy()):\n",
        "            context_index = np.where(text == 50260)[0][0]\n",
        "            labels[i][:context_index+1] = -100\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        b_input_ids = b_input_ids.to(device)\n",
        "        labels = torch.tensor(labels).to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "        \n",
        "            outputs = model(b_input_ids,\n",
        "                        attention_mask=b_masks,\n",
        "                        labels = labels,\n",
        "                        token_type_ids=None)\n",
        "\n",
        "            \n",
        "            loss = outputs[0]\n",
        "\n",
        "            batch_loss = loss\n",
        "\n",
        "            total_train_loss += batch_loss\n",
        "            scaler.scale(batch_loss).backward() \n",
        "            scaler.step(optimizer) \n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)       \n",
        "\n",
        "    print(f'Average Training Loss: {avg_train_loss}.')\n",
        "\n",
        "\n",
        "def validate(val_dataloader, file_name):\n",
        "\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for idx, batch in enumerate(val_dataloader):\n",
        "        b_input_ids = batch[0]\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        labels = b_input_ids.clone().numpy()\n",
        "\n",
        "        for i, text in enumerate(b_input_ids.numpy()):\n",
        "            context_index = np.where(text == 50260)[0][0]\n",
        "            labels[i][:context_index+1] = -100\n",
        "\n",
        "        b_input_ids = b_input_ids.to(device)\n",
        "        labels = torch.tensor(labels).to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs  = model(b_input_ids,  \n",
        "                            attention_mask=b_masks,\n",
        "                            labels=labels)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss\n",
        "        total_eval_loss += batch_loss   \n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "    inference(val_dataloader)\n",
        "\n",
        "    print(f'Validation loss: {avg_val_loss}.')\n",
        "    torch.save(model.state_dict(), '/content/' + file_name)\n",
        "    return model"
      ],
      "metadata": {
        "id": "2ZWk7_WDVNsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch_i in range(0, EPOCHS):\n",
        "    print(f'Epoch {epoch_i + 1} of {EPOCHS}')\n",
        "    train(epoch_i, train_loader)\n",
        "    # validate(val_loader, '/drive/MyDrive/model.pth')"
      ],
      "metadata": {
        "id": "sDWMpiObWIGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_loader)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model.pth'))"
      ],
      "metadata": {
        "id": "6JBfNMy2tOBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "ZOiDDwxTxFhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "metadata": {
        "id": "9DIUNH2lz714"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_input = []\n",
        "master_output = []\n",
        "\n",
        "def getOutputs(val_loader):\n",
        "    model.eval()\n",
        "\n",
        "    for i, batch in enumerate(val_loader):\n",
        "\n",
        "        if i > 0:\n",
        "            lens = np.array([])\n",
        "            input_ids = batch[0].numpy()\n",
        "            attn_masks = batch[1].numpy()\n",
        "\n",
        "            truncated_input = []\n",
        "            truncated_attention_mask = []\n",
        "            for i, input_id in enumerate(input_ids):\n",
        "                context_index = np.where(input_id == 50260)[0][0]\n",
        "                truncated_input.append(input_id[:context_index+1])\n",
        "                truncated_attention_mask.append(attn_masks[i][:context_index+1])\n",
        "                lens = np.append(lens, context_index+1)\n",
        "    \n",
        "            max_len = int(np.amax(lens))\n",
        "            master_input.append(truncated_input)\n",
        "            padded_tokens = []\n",
        "            for tok_ids in truncated_input:\n",
        "                padded_tokens.append(list(tok_ids) + [0] * (max_len - len(tok_ids)))\n",
        "                \n",
        "            padded_tokens = torch.LongTensor(padded_tokens).to(device)\n",
        "            attn_mask = np.zeros(padded_tokens.shape)\n",
        "            \n",
        "            for ix, lengths in enumerate(lens):\n",
        "                attn_mask[ix][:int(lengths)] = 1\n",
        "\n",
        "            attn_mask = torch.tensor(attn_mask).long().to(device)\n",
        "\n",
        "            story_ids = model.generate(padded_tokens, attention_mask=attn_mask,\n",
        "                                num_beams=5,\n",
        "                                max_length=800,\n",
        "                                temperature=1,\n",
        "                                remove_invalid_values = True,\n",
        "                                top_k=50,\n",
        "                                do_sample=True)\n",
        "\n",
        "            raw_stories = [tokenizer.decode(story) for story in story_ids]\n",
        "            master_output.append(raw_stories)\n",
        "\n",
        "    return master_output"
      ],
      "metadata": {
        "id": "soCnEFuQtK8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stories = getOutputs(val_loader)"
      ],
      "metadata": {
        "id": "oXErFneDyltw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['input'] = mmInput\n",
        "df['output'] = master_input"
      ],
      "metadata": {
        "id": "C-FOfIYSyY4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('GPT2_Stories.csv')"
      ],
      "metadata": {
        "id": "RruNUgNP0CsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-DKEZDpWNHaX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}